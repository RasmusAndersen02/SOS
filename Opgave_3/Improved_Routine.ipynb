{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef93357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.lemmatize import \n",
    "\n",
    "df_file = pd.read_csv('korpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581b502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer, Stopword removal. Punctuation remover\n",
    "stop_words = set(stopwords.words('danish'))\n",
    "df_tokens = pd.DataFrame(columns=['file', 'clean_token'])\n",
    "\n",
    "# IN: String, OUT = Lst\n",
    "def TextPrep (file, content, df_tokens):\n",
    "    # content = input['content']\n",
    "    # file = input['file']\n",
    "    \n",
    "    punct_removal = re.sub(r\"[^\\w\\s]\", \" \", content.lower())\n",
    "    token_lst = word_tokenize(punct_removal)\n",
    "    token_lst_swrm =[token for token in token_lst if token not in stop_words]\n",
    "\n",
    "    append_rows = pd.DataFrame({'file': [file] * len(token_lst_swrm), 'clean_token': token_lst_swrm})\n",
    "\n",
    "    return pd.concat([df_tokens, append_rows], ignore_index=True)\n",
    "\n",
    "for file in df_file.iterrows():\n",
    "    file_name = file[1]['file']\n",
    "    file_content = file[1]['content']\n",
    "    df_tokens = TextPrep(file_name, file_content, df_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5908698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lemmy\n",
    "lemmyier = lemmy.load('da')\n",
    "\n",
    "def Lemmatizer (df_tokens):\n",
    "    df_tokens['lemmatized_tokens'] = df_tokens['clean_token'].apply(lambda x: lemmyier.lemmatize('', x)[0])\n",
    "    return df_tokens\n",
    "\n",
    "df_tokens = Lemmatizer(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "tf = df_tokens.groupby(['file', 'lemmatized_tokens']).size().reset_index(name='tf')\n",
    "n = df_tokens.groupby('lemmatized_tokens')['file'].nunique().reset_index(name='n')\n",
    "n_all = df_tokens['file'].nunique().reset_index(name='n_all')\n",
    "N = df_tokens['file'].nunique()\n",
    "\n",
    "n_dict = dict(zip(n['lemmatized_tokens'], n['n']))  \n",
    "idf = {token: math.log(N / n_dict[token], 10) for token in n_dict}\n",
    "\n",
    "tfidf_df = tf.merge(n, on='lemmatized_tokens')\n",
    "tfidf_df['tfidf'] = tfidf_df.apply(\n",
    "    lambda row: row['tf'] * math.log((N+1) / row['n'] + 1, 10),\n",
    "    axis=1\n",
    ")\n",
    "df_tokens = df_tokens.merge(tfidf_df, on=['file', 'lemmatized_tokens'], how='left')\n",
    "df_tokens = df_tokens.drop_duplicates(subset=['file', 'lemmatized_tokens'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1073fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tormand86/hunspell-danish\n",
    "import enchant\n",
    "dk_dict = enchant.Dict('da_DK')\n",
    "\n",
    "def CompoundSplitter(token, recursion_depth = 2):\n",
    "    splits = []\n",
    "    for i in range(3, len(token) - 3):  \n",
    "        fst, snd = token[:i], token[i:]\n",
    "        if dk_dict.check(fst) and dk_dict.check(snd):\n",
    "            splits.append([recursion_depth, fst])\n",
    "            splits.append([recursion_depth, snd])\n",
    "\n",
    "            recurse_snd = CompoundSplitter(snd, recursion_depth + 1)\n",
    "            if recurse_snd:\n",
    "                splits.extend(recurse_snd)\n",
    "\n",
    "            recurse_fst = CompoundSplitter(fst, recursion_depth + 1)\n",
    "            if recurse_fst:\n",
    "                splits.extend(recurse_fst)\n",
    "\n",
    "    return splits if splits else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "703cd602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveDupes(splits):\n",
    "    unique_splits = {}\n",
    "    for split in splits:\n",
    "        # Key is the tuple of the words (ignore depth)\n",
    "        key = tuple(split[1:])\n",
    "        depth = split[0]\n",
    "        # Keep if new or if smaller depth than existing\n",
    "        if key not in unique_splits or depth < unique_splits[key][0]:\n",
    "            unique_splits[key] = split\n",
    "    # Return list of unique splits\n",
    "    return list(unique_splits.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea5e1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitSorter(token):\n",
    "    split = CompoundSplitter(token)\n",
    "    if split is None:\n",
    "        split = []\n",
    "    split.append([1, token])\n",
    "    try:\n",
    "        split = RemoveDupes(split)\n",
    "        sorted_split = sorted(split, key=lambda x: x[0])\n",
    "        return sorted_split\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "df_tokens['root'] = df_tokens['lemmatized_tokens'].apply(lambda x: SplitSorter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9aa6e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'binyrebarkhormonpræparat'],\n",
       " [2, 'binyre'],\n",
       " [2, 'barkhormonpræparat'],\n",
       " [2, 'hormonpræparat'],\n",
       " [2, 'præparat'],\n",
       " [2, 'binyrebark'],\n",
       " [2, 'binyrebarkhormon'],\n",
       " [3, 'bark'],\n",
       " [3, 'hor'],\n",
       " [3, 'monpræparat'],\n",
       " [3, 'præ'],\n",
       " [3, 'parat'],\n",
       " [3, 'hormon'],\n",
       " [3, 'hormonpræ'],\n",
       " [3, 'barkhor'],\n",
       " [3, 'barkhormon'],\n",
       " [3, 'biny'],\n",
       " [3, 'rebark'],\n",
       " [4, 'mon'],\n",
       " [4, 'monpræ']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [['medicin', 'gruppe'], ['medicing', 'ruppe'], ['med', 'icing']]\n",
    "\n",
    "\n",
    "#[['binyre', 'barkhormonpræparat'], ['bark', 'hormonpræparat']]\n",
    "\n",
    "SplitSorter('binyrebarkhormonpræparat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d28aacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, 'risiko'] [2, 'risiko']\n",
      "2\n",
      "[1, 'risikofaktor'] [1, 'risikofaktor']\n",
      "1\n",
      "[1, 'faktor'] [2, 'faktor']\n",
      "2\n",
      "[1, 'risiko'] [2, 'risiko']\n",
      "2\n",
      "[1, 'faktor'] [2, 'faktor']\n",
      "2\n",
      "[1, 'risikofaktor'] [1, 'risikofaktor']\n",
      "1\n",
      "[1, 'risiko'] [2, 'risiko']\n",
      "2\n",
      "[1, 'risiko'] [2, 'risiko']\n",
      "2\n",
      "[1, 'risiko'] [2, 'risiko']\n",
      "2\n",
      "[1, 'risikofaktor'] [1, 'risikofaktor']\n",
      "1\n",
      "[1, 'risiko'] [2, 'risiko']\n",
      "2\n",
      "[1, 'risiko'] [2, 'risiko']\n",
      "2\n",
      "[1, 'risiko'] [2, 'risiko']\n",
      "2\n",
      "[1, 'risiko'] [2, 'risiko']\n",
      "2\n",
      "[1, 'risiko'] [2, 'risiko']\n",
      "2\n",
      "[1, 'risiko'] [2, 'risiko']\n",
      "2\n",
      "[1, 'faktor'] [2, 'faktor']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def KeywordSearch(keyword, df_tokens):\n",
    "\n",
    "    keyword_components = SplitSorter(keyword)\n",
    "    hit_rows = []\n",
    "    \n",
    "    dumbass_breaker = False\n",
    "    for i, row in df_tokens.iterrows():\n",
    "        for key_comp in keyword_components:\n",
    "            for root_comp in row['root']:\n",
    "                if key_comp[1] == root_comp[1]:\n",
    "                    print(root_comp, key_comp)\n",
    "\n",
    "                    divisor =  root_comp[0] * key_comp[0]\n",
    "                    print(divisor)\n",
    "                    hit_score = (row['tfidf']) / divisor\n",
    "\n",
    "                    hit_rows.append({\n",
    "                        'index': i, \n",
    "                        'file': row['file'], \n",
    "                        'search word': row['lemmatized_tokens'], \n",
    "                        'hitword': row['root'][0][1],\n",
    "                        'key_component': key_comp[1], \n",
    "                        'root_component': root_comp[1], \n",
    "                        'tf_idf': row['tfidf'], \n",
    "                        'tf_idf_adjusted': hit_score})\n",
    "                    dumbass_breaker = True\n",
    "                    break\n",
    "            if dumbass_breaker:\n",
    "                dumbass_breaker = False\n",
    "                break\n",
    "\n",
    "\n",
    "    return pd.DataFrame(hit_rows)\n",
    "\n",
    "def ExpressionDecomp(search, df_tokens):\n",
    "\n",
    "    df_all_hits = pd.DataFrame()\n",
    "    search_punct_removal = re.sub(r\"[^\\w\\s]\", \" \", search.lower())\n",
    "    search_to_token_lst = word_tokenize(search_punct_removal)\n",
    "    keywords =[token for token in search_to_token_lst if token not in stop_words]\n",
    "\n",
    "    for keyword in keywords:\n",
    "        df_temp = KeywordSearch(keyword, df_tokens)\n",
    "        df_all_hits = pd.concat([df_all_hits, df_temp])\n",
    "    return df_all_hits\n",
    "        \n",
    "\n",
    "                \n",
    "a =ExpressionDecomp('type-2 risikofaktor', df_tokens)    \n",
    "    \n",
    "def SearchResult (df_hits):\n",
    "    return (df_hits.groupby('file')['tf_idf_adjusted'].mean())\n",
    "\n",
    "\n",
    "b = SearchResult(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa911ed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
