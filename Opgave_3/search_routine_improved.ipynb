{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef93357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.lemmatize import \n",
    "\n",
    "df_file = pd.read_csv('korpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer, Stopword removal. Punctuation remover\n",
    "stop_words = set(stopwords.words('danish'))\n",
    "df_tokens = pd.DataFrame(columns=['file', 'clean_token'])\n",
    "\n",
    "# IN: String, OUT = Lst\n",
    "def TextPrep (file, content, df_tokens):\n",
    "    # content = input['content']\n",
    "    # file = input['file']\n",
    "    \n",
    "    punct_removal = re.sub(r\"[^\\w\\s]\", \" \", content.lower())\n",
    "    token_lst = word_tokenize(punct_removal)\n",
    "    token_lst_swrm =[token for token in token_lst if token not in stop_words]\n",
    "\n",
    "    append_rows = pd.DataFrame({'file': [file] * len(token_lst_swrm), 'clean_token': token_lst_swrm})\n",
    "\n",
    "    return pd.concat([df_tokens, append_rows], ignore_index=True)\n",
    "\n",
    "for file in df_file.iterrows():\n",
    "    file_name = file[1]['file']\n",
    "    file_content = file[1]['content']\n",
    "    df_tokens = TextPrep(file_name, file_content, df_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5908698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lemmy\n",
    "lemmyier = lemmy.load('da')\n",
    "\n",
    "def Lemmatizer (df_tokens):\n",
    "    df_tokens['lemmatized_tokens'] = df_tokens['clean_token'].apply(lambda x: lemmyier.lemmatize('', x)[0])\n",
    "    return df_tokens\n",
    "\n",
    "df_tokens = Lemmatizer(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aafd42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "tf = df_tokens.groupby(['file', 'lemmatized_tokens']).size().reset_index(name='tf')\n",
    "n = df_tokens.groupby('lemmatized_tokens')['file'].nunique().reset_index(name='n')\n",
    "N = df_tokens['file'].nunique()\n",
    "\n",
    "n_dict = dict(zip(n['lemmatized_tokens'], n['n']))  \n",
    "idf = {token: math.log(N / n_dict[token], 10) for token in n_dict}\n",
    "\n",
    "tfidf_df = tf.merge(n, on='lemmatized_tokens')\n",
    "tfidf_df['tfidf'] = tfidf_df.apply(\n",
    "    lambda row: row['tf'] * math.log((N+1) / row['n'] + 1, 10),\n",
    "    axis=1\n",
    ")\n",
    "df_tokens = df_tokens.merge(tfidf_df, on=['file', 'lemmatized_tokens'], how='left')\n",
    "df_tokens = df_tokens.drop_duplicates(subset=['file', 'lemmatized_tokens'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1073fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tormand86/hunspell-danish\n",
    "import enchant\n",
    "dk_dict = enchant.Dict('da_DK')\n",
    "\n",
    "def CompoundSplitter(token, recursion_depth = 2):\n",
    "    splits = []\n",
    "    for i in range(3, len(token) - 3):  \n",
    "        fst, snd = token[:i], token[i:]\n",
    "        if dk_dict.check(fst) and dk_dict.check(snd):\n",
    "            splits.append([recursion_depth, fst])\n",
    "            splits.append([recursion_depth, snd])\n",
    "\n",
    "            recurse_snd = CompoundSplitter(snd, recursion_depth + 1)\n",
    "            if recurse_snd:\n",
    "                splits.extend(recurse_snd)\n",
    "\n",
    "            recurse_fst = CompoundSplitter(fst, recursion_depth + 1)\n",
    "            if recurse_fst:\n",
    "                splits.extend(recurse_fst)\n",
    "\n",
    "    return splits if splits else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703cd602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveDupes(splits):\n",
    "    unique_splits = {}\n",
    "    for split in splits:\n",
    "        # Key is the tuple of the words (ignore depth)\n",
    "        key = tuple(split[1:])\n",
    "        depth = split[0]\n",
    "        # Keep if new or if smaller depth than existing\n",
    "        if key not in unique_splits or depth < unique_splits[key][0]:\n",
    "            unique_splits[key] = split\n",
    "    # Return list of unique splits\n",
    "    return list(unique_splits.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5e1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitSorter(token):\n",
    "    split = CompoundSplitter(token)\n",
    "    if split is None:\n",
    "        split = []\n",
    "    split.append([1, token])\n",
    "    try:\n",
    "        split = RemoveDupes(split)\n",
    "        sorted_split = sorted(split, key=lambda x: x[0])\n",
    "        return sorted_split\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "df_tokens['root'] = df_tokens['lemmatized_tokens'].apply(lambda x: SplitSorter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9aa6e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'binyrebarkhormonpræparat'],\n",
       " [2, 'binyre'],\n",
       " [2, 'barkhormonpræparat'],\n",
       " [2, 'hormonpræparat'],\n",
       " [2, 'præparat'],\n",
       " [2, 'binyrebark'],\n",
       " [2, 'binyrebarkhormon'],\n",
       " [3, 'bark'],\n",
       " [3, 'hor'],\n",
       " [3, 'monpræparat'],\n",
       " [3, 'præ'],\n",
       " [3, 'parat'],\n",
       " [3, 'hormon'],\n",
       " [3, 'hormonpræ'],\n",
       " [3, 'barkhor'],\n",
       " [3, 'barkhormon'],\n",
       " [3, 'biny'],\n",
       " [3, 'rebark'],\n",
       " [4, 'mon'],\n",
       " [4, 'monpræ']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [['medicin', 'gruppe'], ['medicing', 'ruppe'], ['med', 'icing']]\n",
    "\n",
    "\n",
    "#[['binyre', 'barkhormonpræparat'], ['bark', 'hormonpræparat']]\n",
    "\n",
    "SplitSorter('binyrebarkhormonpræparat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d28aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KeywordSearch(keyword, df_tokens):\n",
    "\n",
    "    keyword_components = SplitSorter(keyword)\n",
    "    hit_rows = []\n",
    "    \n",
    "    dumbass_breaker = False\n",
    "    for i, row in df_tokens.iterrows():\n",
    "        for key_comp in keyword_components:\n",
    "            for root_comp in row['root']:\n",
    "                if key_comp[1] == root_comp[1]:\n",
    "                    print(root_comp, key_comp)\n",
    "\n",
    "                    divisor =  root_comp[0] * key_comp[0]\n",
    "                    print(divisor)\n",
    "                    hit_score = (row['tfidf']) / divisor\n",
    "\n",
    "                    hit_rows.append({\n",
    "                        'index': i, \n",
    "                        'file': row['file'], \n",
    "                        'search word': row['lemmatized_tokens'], \n",
    "                        'hitword': row['root'][0][1],\n",
    "                        'key_component': key_comp[1], \n",
    "                        'root_component': root_comp[1], \n",
    "                        'tf_idf': row['tfidf'], \n",
    "                        'tf_idf_adjusted': hit_score})\n",
    "                    dumbass_breaker = True\n",
    "                    break\n",
    "            if dumbass_breaker:\n",
    "                dumbass_breaker = False\n",
    "                break\n",
    "\n",
    "\n",
    "    return pd.DataFrame(hit_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71a1594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[1, 'sår'] [1, 'sår']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ExpressionDecomp(search, df_tokens):\n",
    "\n",
    "    df_all_hits = pd.DataFrame()\n",
    "    search_punct_removal = re.sub(r\"[^\\w\\s]\", \" \", search.lower())\n",
    "    search_to_token_lst = word_tokenize(search_punct_removal)\n",
    "    keywords =[token for token in search_to_token_lst if token not in stop_words]\n",
    "\n",
    "    for keyword in keywords:\n",
    "        df_temp = KeywordSearch(keyword, df_tokens)\n",
    "        df_all_hits = pd.concat([df_all_hits, df_temp])\n",
    "    return df_all_hits\n",
    "        \n",
    "a = ExpressionDecomp('sår diabetes', df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd582cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[1, 'sår'] [1, 'sår']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "def SearchResult (quary, df_tokens):\n",
    "    df_hits = ExpressionDecomp(quary, df_tokens)\n",
    "    return (df_hits.groupby('file')['tf_idf_adjusted'].sum())\n",
    "\n",
    "b = SearchResult('sår diabetes', df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b540798",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = {\n",
    "    'sår diabetes': {\n",
    "        'Hælsår.txt',\n",
    "        'Neuropatiske og neuroiskæmiske sår.txt',\n",
    "        'Charcotfod.txt',\n",
    "        'Diabetisk neuropati.txt',\n",
    "        'Behandling for diabetes 2.txt',\n",
    "        'Netdoktor.txt'\n",
    "    },\n",
    "    'livsstilssygdom': {\n",
    "        'Livsstil.txt',\n",
    "        'Risiko.txt',\n",
    "        'Behandling for diabetes 2.txt',\n",
    "        'Motion.txt',\n",
    "        'Mad.txt',\n",
    "        'Diabetes hos børn.txt'\n",
    "    },\n",
    "    'type 2-diabetes': {\n",
    "        'Behandling for diabetes 2.txt',\n",
    "        'Risiko.txt',\n",
    "        'Motion.txt',\n",
    "        'Mad.txt',\n",
    "        'Livsstil.txt',\n",
    "        'Diabetisk neuropati.txt'\n",
    "    },\n",
    "    'kost sukkersyge': {\n",
    "        'Mad.txt',\n",
    "        'Behandling for diabetes 2.txt',\n",
    "        'Livsstil.txt',\n",
    "        'Risiko.txt',\n",
    "        'Motion.txt',\n",
    "        'Diabetes hos børn.txt'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "014d6f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[2, 'sår'] [1, 'sår']\n",
      "2\n",
      "[1, 'sår'] [1, 'sår']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'sygdom'] [2, 'sygdom']\n",
      "4\n",
      "[2, 'livs'] [3, 'livs']\n",
      "6\n",
      "[2, 'livs'] [3, 'livs']\n",
      "6\n",
      "[2, 'sygdom'] [2, 'sygdom']\n",
      "4\n",
      "[2, 'livs'] [3, 'livs']\n",
      "6\n",
      "[2, 'livs'] [3, 'livs']\n",
      "6\n",
      "[1, 'sygdom'] [2, 'sygdom']\n",
      "2\n",
      "[1, 'sygdom'] [2, 'sygdom']\n",
      "2\n",
      "[2, 'sygdom'] [2, 'sygdom']\n",
      "4\n",
      "[3, 'stils'] [3, 'stils']\n",
      "9\n",
      "[2, 'sygdom'] [2, 'sygdom']\n",
      "4\n",
      "[1, 'livsstilssygdom'] [1, 'livsstilssygdom']\n",
      "1\n",
      "[2, 'livs'] [3, 'livs']\n",
      "6\n",
      "[2, 'livs'] [3, 'livs']\n",
      "6\n",
      "[1, 'sygdom'] [2, 'sygdom']\n",
      "2\n",
      "[1, 'sygdom'] [2, 'sygdom']\n",
      "2\n",
      "[2, 'livs'] [3, 'livs']\n",
      "6\n",
      "[1, 'sygdom'] [2, 'sygdom']\n",
      "2\n",
      "[2, 'sygdom'] [2, 'sygdom']\n",
      "4\n",
      "[2, 'sygdom'] [2, 'sygdom']\n",
      "4\n",
      "[2, 'livs'] [3, 'livs']\n",
      "6\n",
      "[1, 'sygdom'] [2, 'sygdom']\n",
      "2\n",
      "[2, 'sygdom'] [2, 'sygdom']\n",
      "4\n",
      "[2, 'sygdom'] [2, 'sygdom']\n",
      "4\n",
      "[2, 'livs'] [3, 'livs']\n",
      "6\n",
      "[1, 'sygdom'] [2, 'sygdom']\n",
      "2\n",
      "[2, 'sygdom'] [2, 'sygdom']\n",
      "4\n",
      "[2, 'livs'] [3, 'livs']\n",
      "6\n",
      "[2, 'livs'] [3, 'livs']\n",
      "6\n",
      "[2, 'sygdom'] [2, 'sygdom']\n",
      "4\n",
      "[2, 'livsstils'] [2, 'livsstils']\n",
      "4\n",
      "[1, 'sygdom'] [2, 'sygdom']\n",
      "2\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[2, 'type'] [1, 'type']\n",
      "2\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[1, 'type'] [1, 'type']\n",
      "1\n",
      "[2, 'type'] [1, 'type']\n",
      "2\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, '2'] [1, '2']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[1, 'diabetes'] [1, 'diabetes']\n",
      "1\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[2, 'diabetes'] [1, 'diabetes']\n",
      "2\n",
      "[1, 'kost'] [1, 'kost']\n",
      "1\n",
      "[2, 'kost'] [1, 'kost']\n",
      "2\n",
      "[2, 'kost'] [1, 'kost']\n",
      "2\n",
      "[2, 'kost'] [1, 'kost']\n",
      "2\n",
      "[2, 'kost'] [1, 'kost']\n",
      "2\n",
      "[1, 'kost'] [1, 'kost']\n",
      "1\n",
      "[2, 'sukker'] [2, 'sukker']\n",
      "4\n",
      "[1, 'sukker'] [2, 'sukker']\n",
      "2\n",
      "[1, 'sukkersyge'] [1, 'sukkersyge']\n",
      "1\n",
      "[2, 'sukker'] [2, 'sukker']\n",
      "4\n",
      "[1, 'sukker'] [2, 'sukker']\n",
      "2\n",
      "[2, 'sukker'] [2, 'sukker']\n",
      "4\n",
      "[2, 'sukker'] [2, 'sukker']\n",
      "4\n",
      "[1, 'sukkersyge'] [1, 'sukkersyge']\n",
      "1\n",
      "[2, 'sukker'] [2, 'sukker']\n",
      "4\n",
      "[2, 'sukker'] [2, 'sukker']\n",
      "4\n",
      "[3, 'sukker'] [2, 'sukker']\n",
      "6\n",
      "[2, 'sukker'] [2, 'sukker']\n",
      "4\n",
      "[3, 'sukker'] [2, 'sukker']\n",
      "6\n",
      "[2, 'sukker'] [2, 'sukker']\n",
      "4\n",
      "[2, 'sukker'] [2, 'sukker']\n",
      "4\n",
      "[3, 'sukker'] [2, 'sukker']\n",
      "6\n",
      "[2, 'sukker'] [2, 'sukker']\n",
      "4\n",
      "[2, 'sukker'] [2, 'sukker']\n",
      "4\n",
      "[1, 'sukker'] [2, 'sukker']\n",
      "2\n",
      "[2, 'sukkersyge'] [1, 'sukkersyge']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "def Searcher (quary, df_tokens):\n",
    "    result = SearchResult(quary, df_tokens)\n",
    "    result_sort = result.sort_values(ascending=False)\n",
    "    threshhold = result_sort.mean()\n",
    "\n",
    "    result_thresh = result_sort[result_sort >= threshhold] \n",
    "    retrieved_docs = list(result_thresh.index)  # all files that had hits\n",
    "    return retrieved_docs\n",
    "\n",
    "for quary in gold_standard:\n",
    "    results[quary] = Searcher(quary, df_tokens)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c64754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def EvalQuary(gold_set, retrieved_list):\n",
    "    retrieved_clean = [doc for doc in retrieved_list if doc is not None]\n",
    "\n",
    "    # Union of all docs involved\n",
    "    comparison_set = list(set(gold_set) | set(retrieved_clean))\n",
    "\n",
    "    gold_vector = [1 if doc in gold_set else 0 for doc in comparison_set]\n",
    "    retrieved_vector = [1 if doc in retrieved_clean else 0 for doc in comparison_set]\n",
    "\n",
    "    precision = precision_score(gold_vector, retrieved_vector, zero_division=0)\n",
    "    recall = recall_score(gold_vector, retrieved_vector, zero_division=0)\n",
    "    f1 = f1_score(gold_vector, retrieved_vector, zero_division=0)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bf81067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sår diabetes: Precision=0.50, Recall=0.33, F1=0.40\n",
      "livsstilssygdom: Precision=0.60, Recall=0.50, F1=0.55\n",
      "type 2-diabetes: Precision=0.75, Recall=0.50, F1=0.60\n",
      "kost sukkersyge: Precision=1.00, Recall=0.50, F1=0.67\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for query in gold_standard:\n",
    "    gold = gold_standard[query]\n",
    "    retrieved = results.get(query, [])[:6] \n",
    "\n",
    "    precision, recall, f1 = EvalQuary(gold, retrieved)\n",
    "    scores.append((query, precision, recall, f1))\n",
    "\n",
    "    print(f\"{query}: Precision={precision:.2f}, Recall={recall:.2f}, F1={f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85728b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "119f54c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Scores (Top 6 only):\n",
      "Precision: 0.71\n",
      "Recall:    0.46\n",
      "F1 Score:  0.55\n"
     ]
    }
   ],
   "source": [
    "avg_precision = np.mean([s[1] for s in scores])\n",
    "avg_recall = np.mean([s[2] for s in scores])\n",
    "avg_f1 = np.mean([s[3] for s in scores])\n",
    "\n",
    "print(\"\\nAverage Scores (Top 6 only):\")\n",
    "print(f\"Precision: {avg_precision:.2f}\")\n",
    "print(f\"Recall:    {avg_recall:.2f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5d044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
